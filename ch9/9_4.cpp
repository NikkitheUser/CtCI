#include <iostream>
/*
Duplicate URLS: you have 10 billion urls. how do we detect the duplicate docs? assume duplicate refers to being identical 

this is gonna be large. we can assume that this will take between 3-4 terabytes of data. so we are going to have to power through that fact if we want to figure out the best method to figure out this problem. 

in a simple senario, where the amount of data isnt an issue, we can use disk storage as a solution. 
store all the data onto one machine, we can pass through the documentation twice. first rounf would be for splitting the urls by hash data. 
the second round will be to actually find the duplicate data.

  */
int main() {
  std::cout << "if you want explinations, read the commented out portion.";
}
